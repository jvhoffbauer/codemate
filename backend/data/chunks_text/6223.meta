{"original_code": "@cached(cache=TTLCache(maxsize=4, ttl=1800))\nasync def get_category(category):\n    \"\"\"\n    Retrieves the data for the provided category. The data is cached for 30 minutes locally, 1 hour via shared Redis.\n\n    :returns: The data for category.\n    :rtype: dict\n    \"\"\"\n\n    # Adhere to category naming standard.\n    category = category.lower()\n    data_id = f\"jhu.{category}\"\n\n    # check shared cache\n    cache_results = await check_cache(data_id)\n    if cache_results:\n        LOGGER.info(f\"{data_id} using shared cache results\")\n        results = cache_results\n    else:\n        LOGGER.info(f\"{data_id} shared cache empty\")\n        # URL to request data from.\n        url = BASE_URL + \"time_series_covid19_%s_global.csv\" % category\n\n        # Request the data\n        LOGGER.info(f\"{data_id} Requesting data...\")\n        async with httputils.CLIENT_SESSION.get(url) as response:\n            text = await response.text()\n\n        LOGGER.debug(f\"{data_id} Data received\")\n\n        # Parse the CSV.\n        data = list(csv.DictReader(text.splitlines()))\n        LOGGER.debug(f\"{data_id} CSV parsed\")\n\n        # The normalized locations.\n        locations = []\n\n        for item in data:\n            # Filter out all the dates.\n            dates = dict(\n                filter(lambda element: date_util.is_date(element[0]), item.items())\n            )\n\n            # Make location history from dates.\n            history = {date: int(float(amount or 0)) for date, amount in dates.items()}\n\n            # Latest data insert value.\n            latest = list(history.values())[-1]\n\n            # Country for this location.\n            country = item[\"Country/Region\"]\n\n            # Normalize the item and append to locations.\n            locations.append(\n                {\n                    \"country\": country,\n                    \"country_code\": countries.country_code(country),\n                    \"province\": item[\"Province/State\"],\n                    \"coordinates\": {\n                        \"lat\": item[\"Lat\"],\n                        \"long\": item[\"Long\"],\n                    },\n                    \"history\": history,\n                    \"latest\": int(latest or 0),\n                }\n            )\n        LOGGER.debug(f\"{data_id} Data normalized\")\n\n        # Latest total.\n        latest = sum(map(lambda location: location[\"latest\"], locations))\n\n        # Return the final data.\n        results = {\n            \"locations\": locations,\n            \"latest\": latest,\n            \"last_updated\": datetime.utcnow().isoformat() + \"Z\",\n            \"source\": \"https://github.com/ExpDev07/coronavirus-tracker-api\",\n        }\n        # save the results to distributed cache\n        await load_cache(data_id, results)\n\n    LOGGER.info(f\"{data_id} results:\\n{pf(results, depth=1)}\")\n    return results"}